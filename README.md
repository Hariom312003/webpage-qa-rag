
# ðŸŒ Webpage Q&A using RAG and Ollama

A full-stack AI project that enables users to ask natural language questions
about **any open webpage** using a **Chrome Extension** powered by
**Retrieval-Augmented Generation (RAG)** and a **local LLM (LLaMA-3 via Ollama)**.

This system generates **accurate, hallucination-free answers** strictly based
on the webpage content â€” without using any paid cloud APIs.

---

## ðŸš€ Key Features

- ðŸ” Ask questions about any webpage in real time
- ðŸ§  Retrieval-Augmented Generation (RAG) architecture
- ðŸ’» Fully local LLM using Ollama (LLaMA-3)
- âš¡ Chrome Extension (Manifest V3)
- ðŸ§© Efficient text chunking and semantic search
- ðŸ›¡ï¸ Answers grounded strictly in webpage content

---

## ðŸ§  Why This Project?

Traditional LLMs often hallucinate or use external knowledge.
This project solves that problem using **RAG**, ensuring:

- Answers come only from webpage data
- Transparent and explainable responses
- No dependency on OpenAI or paid APIs
- Privacy-friendly, offline-capable setup

---

## ðŸ—ï¸ System Architecture

Webpage  
â†“  
Chrome Extension (DOM Extraction)  
â†“  
FastAPI Backend  
â†“  
Text Cleaning & Chunking  
â†“  
Embedding Generation  
â†“  
ChromaDB (Vector Store)  
â†“  
Relevant Context Retrieval  
â†“  
Ollama (LLaMA-3)  
â†“  
Answer to User

---

## ðŸ”„ End-to-End Pipeline

1. User opens any webpage
2. Chrome Extension extracts visible text
3. User clicks **Scan Page**
4. Backend:
   - Cleans and chunks text
   - Converts chunks into embeddings
5. ChromaDB stores vectors
6. User asks a question
7. Backend retrieves relevant chunks
8. Prompt + context sent to LLaMA-3 (Ollama)
9. Grounded answer returned to extension

---

## ðŸ§° Tech Stack

### Backend
- FastAPI
- LangChain
- ChromaDB
- Ollama (LLaMA-3)

### Frontend
- Chrome Extension (Manifest V3)
- HTML, CSS, JavaScript

### AI / ML
- Retrieval-Augmented Generation (RAG)
- Local Embeddings + LLM

---

## ðŸ“ Project Structure

webpage-qa-rag/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ rag_pipeline.py
â”‚ â”œâ”€â”€ vector_store.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ chrome-extension/
â”‚ â”œâ”€â”€ manifest.json
â”‚ â”œâ”€â”€ content.js
â”‚ â”œâ”€â”€ popup.html
â”‚ â”œâ”€â”€ popup.js
â”‚ â”œâ”€â”€ background.js
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md

<<<<<<< HEAD

âš™ï¸ Setup Instructions (Linux):-
 1ï¸âƒ£ Install Ollama:-
      curl -fsSL https://ollama.com/install.sh | sh
    Pull the model:-
      ollama pull llama3
    Run Ollama:-
      ollama serve
2ï¸âƒ£ Backend Setup:-
      cd backend
      python3 -m venv venv
      source venv/bin/activate
      pip install -r requirements.txt
    Run server:-
      uvicorn app:app --reload --port 8001
    Backend runs at:-
       http://127.0.0.1:8001

3ï¸âƒ£ Load Chrome Extension

1.Open Chrome â†’ chrome://extensions

2.Enable Developer mode
=======
---

## âš™ï¸ Setup Instructions (Linux)

### 1ï¸âƒ£ Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3
ollama serve

2ï¸âƒ£ Backend Setup:-
 cd backend
 python3 -m venv venv
 source venv/bin/activate
 pip install -r requirements.txt
 uvicorn app:app --reload --port 8001

Backend runs at:-
  http://127.0.0.1:8001

3ï¸âƒ£ Load Chrome Extension:-

1.Open Chrome â†’ chrome://extensions

2.Enable Developer Mode
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)

3.Click Load unpacked

4.Select chrome-extension/ folder

ðŸ§ª How to Use

1.Open any webpage

2.Click the extension icon

3.Click Scan Page

<<<<<<< HEAD
4.Ask a question like:

  â€œWhat is this page about?â€

  â€œWhat are the main features?â€

5.Get an accurate answer from page content only

ðŸ” Design Decisions

->âŒ No cloud APIs â†’ avoids cost & privacy issues

->âœ… Local LLM â†’ faster iteration, offline support

->âœ… RAG â†’ prevents hallucinations

->âœ… ChromaDB â†’ fast similarity search

ðŸš€ Future Improvements

-> Streaming responses

-> Multi-page memory

-> Page re-scan auto detection

-> Answer citations (highlight source text)

-> Semantic section-wise search

ðŸ‘¨â€ðŸ’» Author:-
=======
4.Ask questions like:

  "What is this page about?"

  "What are the main features?"

5.Get accurate, page-grounded answers

ðŸ” Design Decisions:-

> Local LLM â†’ No cost, privacy-friendly

> RAG architecture â†’ Prevent hallucinations

> ChromaDB â†’ Fast vector search

> Chrome Extension â†’ Seamless UX

ðŸš€ Future Enhancements:-

> Streaming responses

> Highlight answer source text

> Multi-page memory

> Automatic page re-scan

> UI improvements

ðŸ‘¨â€ðŸ’» Author
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)

Hariom Gupta
