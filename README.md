
ðŸŒ Webpage Q&A using RAG and Ollama:-

A Chrome Extension + Backend AI system that allows users to ask natural language questions about any open webpage.
The system uses Retrieval-Augmented Generation (RAG) with a local Large Language Model (LLaMA-3 via Ollama) to generate grounded, hallucination-free answers strictly from webpage content.

This project works fully offline (local LLM) and does not rely on paid APIs.

ðŸš€ Key Features:-

-> ðŸ” Ask questions about any webpage in real time

-> ðŸ§  Uses RAG (Retrieval-Augmented Generation) for accurate answers

-> ðŸ’» Runs locally using Ollama (LLaMA-3) â€” no OpenAI key required

-> âš¡ Chrome Extension with clean UI (Manifest V3)

-> ðŸ§© Efficient text chunking and vector search

-> ðŸ›¡ï¸ Answers are strictly grounded in webpage content

ðŸ§  Why RAG?

Traditional LLMs can hallucinate or use external knowledge.

# RAG fixes this by:

 1.Extracting content from the webpage

 2.Converting it into embeddings

 3.Retrieving only relevant chunks

 4.Sending retrieved context to the LLM

âž¡ï¸ Result: accurate, explainable, context-aware answers

ðŸ—ï¸ System Architecture:-
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Browser  â”‚
â”‚ (Any Webpage) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chrome Extension   â”‚
â”‚ - content.js       â”‚
â”‚ - popup.js         â”‚
â”‚ - background.js    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  (Extracted Text)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI Backend    â”‚
â”‚ - /scan endpoint   â”‚
â”‚ - /ask endpoint    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Pipeline (LangChain)   â”‚
â”‚ - Text Cleaning            â”‚
â”‚ - Chunking                 â”‚
â”‚ - Embeddings               â”‚
â”‚ - Retrieval                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB           â”‚
â”‚ (Vector Store)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ (Top-K Chunks)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama (LLaMA-3)   â”‚
â”‚ Local LLM          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ”„ Complete Pipeline (Step-by-Step)

1:User opens any webpage

2:Chrome Extension extracts visible DOM text

3:User clicks â€œScan Pageâ€

4:Backend:-

  1->Cleans text
  
  2->Splits into chunks

  3->Generates embeddings

5:Embeddings are stored in ChromaDB

6:User asks a question

7:Backend:-

   1->Embeds the question

   2->Retrieves relevant chunks

   3->Builds a prompt with context

8:Ollama (LLaMA-3) generates a grounded answer

9:Answer is shown in the extension UI

ðŸ§° Tech Stack
Backend:-

  1.FastAPI

  2.LangChain

  3.ChromaDB

  4.Ollama (LLaMA-3)

Frontend:-

  1.Chrome Extension (Manifest V3)

  2.HTML, CSS, JavaScript

ML / AI:-

  1.Retrieval-Augmented Generation (RAG)

  2.Local embeddings + LLM

ðŸ“ Project Structure:-
  webpage-qa-rag/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI server
â”‚   â”œâ”€â”€ rag_pipeline.py     # RAG logic
â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB integration
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ chrome-extension/
â”‚   â”œâ”€â”€ manifest.json
â”‚   â”œâ”€â”€ content.js          # Extract webpage text
â”‚   â”œâ”€â”€ popup.html
â”‚   â”œâ”€â”€ popup.js            # UI logic
â”‚   â”œâ”€â”€ background.js
=======
# ðŸŒ Webpage Q&A using RAG and Ollama

A full-stack AI project that enables users to ask natural language questions
about **any open webpage** using a **Chrome Extension** powered by
**Retrieval-Augmented Generation (RAG)** and a **local LLM (LLaMA-3 via Ollama)**.

This system generates **accurate, hallucination-free answers** strictly based
on the webpage content â€” without using any paid cloud APIs.

---

## ðŸš€ Key Features

- ðŸ” Ask questions about any webpage in real time
- ðŸ§  Retrieval-Augmented Generation (RAG) architecture
- ðŸ’» Fully local LLM using Ollama (LLaMA-3)
- âš¡ Chrome Extension (Manifest V3)
- ðŸ§© Efficient text chunking and semantic search
- ðŸ›¡ï¸ Answers grounded strictly in webpage content

---

## ðŸ§  Why This Project?

Traditional LLMs often hallucinate or use external knowledge.
This project solves that problem using **RAG**, ensuring:

- Answers come only from webpage data
- Transparent and explainable responses
- No dependency on OpenAI or paid APIs
- Privacy-friendly, offline-capable setup

---

## ðŸ—ï¸ System Architecture

Webpage  
â†“  
Chrome Extension (DOM Extraction)  
â†“  
FastAPI Backend  
â†“  
Text Cleaning & Chunking  
â†“  
Embedding Generation  
â†“  
ChromaDB (Vector Store)  
â†“  
Relevant Context Retrieval  
â†“  
Ollama (LLaMA-3)  
â†“  
Answer to User

---

## ðŸ”„ End-to-End Pipeline

1. User opens any webpage
2. Chrome Extension extracts visible text
3. User clicks **Scan Page**
4. Backend:
   - Cleans and chunks text
   - Converts chunks into embeddings
5. ChromaDB stores vectors
6. User asks a question
7. Backend retrieves relevant chunks
8. Prompt + context sent to LLaMA-3 (Ollama)
9. Grounded answer returned to extension

---

## ðŸ§° Tech Stack

### Backend
- FastAPI
- LangChain
- ChromaDB
- Ollama (LLaMA-3)

### Frontend
- Chrome Extension (Manifest V3)
- HTML, CSS, JavaScript

### AI / ML
- Retrieval-Augmented Generation (RAG)
- Local Embeddings + LLM

---

## ðŸ“ Project Structure

webpage-qa-rag/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ rag_pipeline.py
â”‚ â”œâ”€â”€ vector_store.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ chrome-extension/
â”‚ â”œâ”€â”€ manifest.json
â”‚ â”œâ”€â”€ content.js
â”‚ â”œâ”€â”€ popup.html
â”‚ â”œâ”€â”€ popup.js
â”‚ â”œâ”€â”€ background.js
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md

<<<<<<< HEAD

âš™ï¸ Setup Instructions (Linux):-
 1ï¸âƒ£ Install Ollama:-
      curl -fsSL https://ollama.com/install.sh | sh
    Pull the model:-
      ollama pull llama3
    Run Ollama:-
      ollama serve
2ï¸âƒ£ Backend Setup:-
      cd backend
      python3 -m venv venv
      source venv/bin/activate
      pip install -r requirements.txt
    Run server:-
      uvicorn app:app --reload --port 8001
    Backend runs at:-
       http://127.0.0.1:8001

3ï¸âƒ£ Load Chrome Extension

1.Open Chrome â†’ chrome://extensions

2.Enable Developer mode
=======
---

## âš™ï¸ Setup Instructions (Linux)

### 1ï¸âƒ£ Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3
ollama serve

2ï¸âƒ£ Backend Setup:-
 cd backend
 python3 -m venv venv
 source venv/bin/activate
 pip install -r requirements.txt
 uvicorn app:app --reload --port 8001

Backend runs at:-
  http://127.0.0.1:8001

3ï¸âƒ£ Load Chrome Extension:-

1.Open Chrome â†’ chrome://extensions

2.Enable Developer Mode
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)

3.Click Load unpacked

4.Select chrome-extension/ folder

ðŸ§ª How to Use

1.Open any webpage

2.Click the extension icon

3.Click Scan Page

<<<<<<< HEAD
4.Ask a question like:

  â€œWhat is this page about?â€

  â€œWhat are the main features?â€

5.Get an accurate answer from page content only

ðŸ” Design Decisions

->âŒ No cloud APIs â†’ avoids cost & privacy issues

->âœ… Local LLM â†’ faster iteration, offline support

->âœ… RAG â†’ prevents hallucinations

->âœ… ChromaDB â†’ fast similarity search

ðŸš€ Future Improvements

-> Streaming responses

-> Multi-page memory

-> Page re-scan auto detection

-> Answer citations (highlight source text)

-> Semantic section-wise search

ðŸ‘¨â€ðŸ’» Author:-
=======
4.Ask questions like:

  "What is this page about?"

  "What are the main features?"

5.Get accurate, page-grounded answers

ðŸ” Design Decisions:-

> Local LLM â†’ No cost, privacy-friendly

> RAG architecture â†’ Prevent hallucinations

> ChromaDB â†’ Fast vector search

> Chrome Extension â†’ Seamless UX

ðŸš€ Future Enhancements:-

> Streaming responses

> Highlight answer source text

> Multi-page memory

> Automatic page re-scan

> UI improvements

ðŸ‘¨â€ðŸ’» Author
>>>>>>> 19a09a1 (Improve README with detailed architecture, pipeline, and setup)

Hariom Gupta
