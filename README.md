ğŸŒ Webpage Q&A using RAG and Ollama:-

A Chrome Extension + Backend AI system that allows users to ask natural language questions about any open webpage.
The system uses Retrieval-Augmented Generation (RAG) with a local Large Language Model (LLaMA-3 via Ollama) to generate grounded, hallucination-free answers strictly from webpage content.

This project works fully offline (local LLM) and does not rely on paid APIs.

ğŸš€ Key Features:-

-> ğŸ” Ask questions about any webpage in real time

-> ğŸ§  Uses RAG (Retrieval-Augmented Generation) for accurate answers

-> ğŸ’» Runs locally using Ollama (LLaMA-3) â€” no OpenAI key required

-> âš¡ Chrome Extension with clean UI (Manifest V3)

-> ğŸ§© Efficient text chunking and vector search

-> ğŸ›¡ï¸ Answers are strictly grounded in webpage content

ğŸ§  Why RAG?

Traditional LLMs can hallucinate or use external knowledge.

# RAG fixes this by:

 1.Extracting content from the webpage

 2.Converting it into embeddings

 3.Retrieving only relevant chunks

 4.Sending retrieved context to the LLM

â¡ï¸ Result: accurate, explainable, context-aware answers

ğŸ—ï¸ System Architecture:-
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Browser  â”‚
â”‚ (Any Webpage) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chrome Extension   â”‚
â”‚ - content.js       â”‚
â”‚ - popup.js         â”‚
â”‚ - background.js    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  (Extracted Text)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI Backend    â”‚
â”‚ - /scan endpoint   â”‚
â”‚ - /ask endpoint    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Pipeline (LangChain)   â”‚
â”‚ - Text Cleaning            â”‚
â”‚ - Chunking                 â”‚
â”‚ - Embeddings               â”‚
â”‚ - Retrieval                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB           â”‚
â”‚ (Vector Store)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ (Top-K Chunks)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama (LLaMA-3)   â”‚
â”‚ Local LLM          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”„ Complete Pipeline (Step-by-Step)

1:User opens any webpage

2:Chrome Extension extracts visible DOM text

3:User clicks â€œScan Pageâ€

4:Backend:-

  1->Cleans text
  
  2->Splits into chunks

  3->Generates embeddings

5:Embeddings are stored in ChromaDB

6:User asks a question

7:Backend:-

   1->Embeds the question

   2->Retrieves relevant chunks

   3->Builds a prompt with context

8:Ollama (LLaMA-3) generates a grounded answer

9:Answer is shown in the extension UI

ğŸ§° Tech Stack
Backend:-

  1.FastAPI

  2.LangChain

  3.ChromaDB

  4.Ollama (LLaMA-3)

Frontend:-

  1.Chrome Extension (Manifest V3)

  2.HTML, CSS, JavaScript

ML / AI:-

  1.Retrieval-Augmented Generation (RAG)

  2.Local embeddings + LLM

ğŸ“ Project Structure:-
  webpage-qa-rag/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI server
â”‚   â”œâ”€â”€ rag_pipeline.py     # RAG logic
â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB integration
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ chrome-extension/
â”‚   â”œâ”€â”€ manifest.json
â”‚   â”œâ”€â”€ content.js          # Extract webpage text
â”‚   â”œâ”€â”€ popup.html
â”‚   â”œâ”€â”€ popup.js            # UI logic
â”‚   â”œâ”€â”€ background.js
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md


âš™ï¸ Setup Instructions (Linux):-
 1ï¸âƒ£ Install Ollama:-
      curl -fsSL https://ollama.com/install.sh | sh
    Pull the model:-
      ollama pull llama3
    Run Ollama:-
      ollama serve
2ï¸âƒ£ Backend Setup:-
      cd backend
      python3 -m venv venv
      source venv/bin/activate
      pip install -r requirements.txt
    Run server:-
      uvicorn app:app --reload --port 8001
    Backend runs at:-
       http://127.0.0.1:8001

3ï¸âƒ£ Load Chrome Extension

1.Open Chrome â†’ chrome://extensions

2.Enable Developer mode

3.Click Load unpacked

4.Select chrome-extension/ folder

ğŸ§ª How to Use

1.Open any webpage

2.Click the extension icon

3.Click Scan Page

4.Ask a question like:

  â€œWhat is this page about?â€

  â€œWhat are the main features?â€

5.Get an accurate answer from page content only

ğŸ” Design Decisions

->âŒ No cloud APIs â†’ avoids cost & privacy issues

->âœ… Local LLM â†’ faster iteration, offline support

->âœ… RAG â†’ prevents hallucinations

->âœ… ChromaDB â†’ fast similarity search

ğŸš€ Future Improvements

-> Streaming responses

-> Multi-page memory

-> Page re-scan auto detection

-> Answer citations (highlight source text)

-> Semantic section-wise search

ğŸ‘¨â€ğŸ’» Author:-

Hariom Gupta
